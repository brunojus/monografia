Neste capítulo é feita uma introdução sobre o \acrshort{AM}, tema principal dessa monografia. E está dividido da seguinte
maneira na seção 2.1 são apresentados os conceitos básicos para o entendimento inicial do tema. Na seção 2.2 são apresentados
os conceitos de mineração de texto. Na seção 2.3 é apresentado o conceito de análise de sentimentos e a sua aplicação.


\section{Aprendizado de Máquina}
  \subsection{Conceitos Básicos}

    Com o alto volume de informações geradas no dia a dia, a utilização de algoritmos que sejam capazes de identificar padrões tornam-se cada
    vez mais necessários, pois em diversas empresas e órgãos do governo não são capazes de analisar todas as informações existentes ou entregar
    de maneira célere \cite{nasrabadi2007pattern}. 
    
    Portanto para realizar esse tipo de atividade é necessário entender o problema existente e também ter um volume de dados razoável para realizar o treinamento do modelo, e com a técnica de \acrshort{AM} é possível automatizar a construção
    de sistemas inteligentes que podem ser ajustados de acordo com a necessidade de cada tarefa \cite{bonaccorso2017machine}.

    Em outras palavras o \acrshort{AM} é um conjunto de regras, que possibilitam uma máquina a tomar decisões baseadas em experiências passadas ao invés
    de um software que teve que ser definido anteriormente como tratar cada tipo de regra, também existe a possibilidade desses modelos serem desenvolvidos
    e melhorarem quando expostos a novos dados \cite{zurada1995review}.


    O conceito de \acrshort{AM} pode ser sintetizado como a capacidade de um programa de computador aprender com a experiencia (E) relacionada a alguma 
    classe de tarefas (T), baseada em uma medida de desempenho (P). Dessa forma, o desempenho em tarefas (T), quando medido por (P), melhora com a 
    experiencia em (E) \cite{mitchell}

    Essas técnicas tem sido utilizadas amplamente para resolução de diversos problemas, atualmente o assunto está em alta e existem diversas oportunidade
    para colocar em prática a matemática que foi desenvolvida para a criação desses algoritmos. Gigantes da indústria tem investido severamente para
    o desenvolvimento de novas tecnologias, como os veículos autônomos, robôs advogados, veículos não tripulados e na identificação de doenças.

    Existem distintos paradigmas para ensinar uma máquina, esse trabalho irá focar apenas nos tipos de \acrshort{AM} que estão sendo utilizados no desenvolvimento do framework,
    portanto não será abordados temas como aprendizado estatístico ou redes bayesianas.

  \subsection{Paradigmas de Aprendizado de Máquina}

    Os principais tipos de \acrshort{AM} utilizados atualmente serão detalhados nas 3 subseções a seguir:

    \subsubsection{Aprendizado Supervisionado}

      O aprendizado supervisionado é uma das formas em ensinar uma tarefa para a máquina, onde existe uma entrada e uma saída desejada que já foi 
      anteriormente rotulada, esse processo pode ser feito de forma manual ou utilizar de dicionários, quando a informação de entrada é um texto.
      Tendo os dados detalhados a máquina é capaz de classificar novas entradas a partir de experiências antigas \cite{mitchell}. Na Figura \ref{supervisionado} é possível visualizar 
      a forma que é realizada etapa de treinamento do modelo, onde é fornecida uma informação, juntamente com o que ela significa, pois esse tipo de aprendizado 
      é necessário rotular todas as informações que serão utilizadas para que a máquina consiga distinguir o que é gato e o que é cachorro.
      

      \figuraBib{supervisionado}{Treinamento de um modelo de aprenzidado de máquina supervisionado}{}{supervisionado}{width=.6\textwidth}%

      Na Figura \ref{supervisionado_predicao} é possível visualizar o modelo preditivo em funcionamento, onde o usuário fornece uma informação de entrada
      e o sistema o responde com a classe que foi identificada através da entrada.

      \figuraBib{supervisionado_predicao}{Utilização do modelo para realizar predição a partir de classes previamente treinadas}{}{supervisionado_predicao}{width=.6\textwidth}%


      Algum dos algoritmos mais utilizados para esse paradigma serão detalhados durante essa subseção.

      \paragraph{SVM}

        A máquina de vetor de suportes, do inglês support vector machine (\acrshort{SVM}), faz parte do aprendizado supervisionado, com ele é possível
        classificar grupos de dados separando as suas margens. Essas margens são delineadas pela fração dos dados de treinamento, são chamadas de vetores de suporte \cite{chang2011libsvm}.

        As vantagens de utilizar \acrshort{SVM} são \cite{pedregosa2011scikit}:

        \begin{itemize}

          \item Efetivos em espaços multidimensionais
          \item Continua eficaz em casos onde o número de dimensões é maior que o número de amostras.
          \item Utiliza os vetores de suporte, que otimiza o uso de memória do computador.
          \item É possível utilizar diversos kernels para a função de decisão.

        \end{itemize}

        Como desvantagens, temos \cite{pedregosa2011scikit}:

        The disadvantages of support vector machines include:

        \begin{itemize}

          \item Se o número de entradas for muito maior que o número de amostras, pois existe a possibilidade de overfitting.

        \end{itemize}
        
        O \acrshort{SVM} é construido em um hiper-plano ou em vários hiper-planos em um espaço de infinitas dimensões, que podem ser
        usadas classificação ou regressão. Na Figura \ref{SVM} é detalhado um hiper-plano que mostra uma boa separação das variáveis
        que possui a maior distância até os pontos dos dados de treinamento mais próximos de qualquer classe, pois é conhecido no \acrshort{SVM}
        que quanto maior for a margem, menor será o erro do classificador\cite{vieira2017plantrna_sniffer}. 
        
        \figuraBib{SVM}{Exemplo de vetores de suporte com dimensão 2}{}{SVM}{width=.6\textwidth}%

        Nesse trabalho foi utilizado esse algoritmo para classificar dados multi-classes, pois foi utilizadas três classes distintas, para
        desenvolvimento do modelo: Positivo, Negativo e Neutro. 
        
        Tendo como os dados de treinamento $x_{i} \in \mathbb{R}^{p}$, onde i$=$1,\cdots, n. Onde $ y \in \left \{ 1,-1 \right \}^{n}$, na Equação \ref{eq:SVM} é
        possível visualizar a solução matemática para esse algoritmo.

        \begin{equation}\label{eq:SVM}
          \begin{aligned}
            & \min_{\omega,\beta, \zeta } \frac{1}{2}\omega^{T}\omega + C \sum_{i=1}^{n}\zeta_{i}\\
            & \textup{sujeito a } y_{i}\left ( \omega^{T}\phi \left ( x_{i} \right ) +b \right )\geq 1 - \zeta_{i},\\
            & \zeta_{i}\geq 0, i = 1,\cdots, n
        \end{aligned}
        \end{equation}

        A partir do espaço de Hilbert, temos que o dual de um espaço de Hilbert é um espaço de Hilbert \cite{lorena2007introduccao}, 
        
        na Equação \ref{eq:svmdual}, temos:

        \begin{equation}\label{eq:svmdual}
          \begin{aligned}
            & \min_{\alpha} \frac{1}{2}\alpha^{T}Q\alpha - e^{T}\alpha\\ \\
            & \textup{sujeito } a y^{T}\alpha = 0 \\ \\
            & 0\leq \alpha_{i}\leq C, i= 1\cdots, n 
        \end{aligned}
        \end{equation}

        onde $e$ é o vetor com todos os valores, $C>0$ é o limite superior, $Q$ é uma matriz semidefinida positiva de tamanho n por n,
        $Q_{ij}\equiv y_{i}y_{j}K(x_{i},x_{j})$, onde $K(x_{i},x_{j}) = \phi(x_{i})^{T}\phi(x_{j})$ é a função kernel do \acrshort{SVM}. Onde os 
        vetores de treinamento são definidos em um espaço dimensional maior pela função $\phi$.
         E por fim na Equação \ref{eq:predSVM}, temos a função de decisão:

         \begin{equation}\label{eq:predSVM}
          \begin{aligned}
            sgn(\sum_{i=1}^{n}y_{i}\alpha_{i}K(x_{i},x)+\rho )
        \end{aligned}
        \end{equation}
        
        Onde o termo $y_{i}\alpha_{i}$ representa o coeficiente dual, $K(x_{i},x)$ são os vetores de suporte e $\rho$ é um termo independente.



        \paragraph{Naive Bayes}

        O Naive Bayes é um outro algoritmo \acrshort{AM} supervisionado, a sua formulação matemática é sustentada pelo teorema estatístico de
        Bayes com um pouco de ingenuidade, pois assume-se que a suposição de independência condicional entre cada par de recursos, dado o valor da
        variável de classe \cite{McCallum98acomparison}. O teorema de Bayes clássico segue a seguinte relação, onde um termo independente $y$ e o vetor $x_{1}$ até $x_{n}$, na Equação
        \ref{eq:Bayes}

        \begin{equation}\label{eq:Bayes}
          \begin{aligned}
            P\left ( y\mid x_{1},\cdots, x_{n} \right ) = \frac{P(y)P(x_{1},\cdots,x_{n}\mid y)}{P(x_{1},\cdots, x_{n})}
        \end{aligned}
        \end{equation}
        

        Na Equação \ref{eq:Bayes1} é desconsiderado completamente a correlação entre as variáveis, 


        \begin{equation}\label{eq:Bayes1}
          \begin{aligned}
            P\left ( x_{i}\mid y,x_{1}, \cdots,x_{i-1},x_{i+1},\cdots, x_{n} \right ) = P(x_{i}|y),
        \end{aligned}
        \end{equation}


        Na Equação \ref{eq:BayesSimples}, para todo $i$, é possível simplificar através da relação:


        \begin{equation}\label{eq:BayesSimples}
          \begin{aligned}
            P(y|x_{1},\cdots, x_{n}) = \frac{P(y)\prod_{i=1}^{n}P(x_{i}|y)}{P(x_{1},\cdots, x_{n})},
        \end{aligned}
        \end{equation}


        Sabendo que $P(x_{1},\cdots, x_{n})$ é constante dada a entrada, podemos usar a seguinte regra de classificação, na Equação \ref{eq:NBFINAL}:
        
        \begin{equation}\label{eq:NBFINAL}
          \begin{aligned}
            \widehat{y} = \arg\max_{y} P(y)\prod_{i=1}^{n}P(x_{i}|y),
        \end{aligned}
        \end{equation}


        O classificador Naive Bayes pode ser extremamente rápido em comparação a outros algoritmos mais sofisticados, pois existe o desacoplamento
        das distribuições de características condicionais de classe que significa que cada uma pode ser estimada independentemente com uma distribuição unidimensional\cite{zhang2004optimality}.


        \paragraph{Árvores de Decisão}

          A árvore de decisão funciona quando as classes presentes no conjunto de dados de treinamento se dividem, ou seja, esse algoritmo seleciona
          um problema mais complexo e divide em subproblemas mais simples e de forma recursiva a mesma estratégia é a aplicada a cada subproblema \cite{coelhoanalise}.


          Na Figura \ref{decisionTree}, é um exemplo de como uma árvore de decisão poderia realizar uma análise de crédito de forma rápida. 


          \figuraBib{decision_tree}{Árvore de decisão para análise de crédito de um cliente que deseja empréstimo alto no banco, levando em consideração 
          a educação e faixa salarial}{}{decisionTree}{width=\textwidth}%


          Tendo como entrada os vetores de treinamento $x_{i} \in \mathbb{R}^{n}$, $i=1,\cdots, n$, e o vetor com os rótulos de saída $y \in \mathbb{R}^{n}$,
          a árvore de decisão divide recursivamente o espaço de forma que as amostras com os mesmos rótulos sejam agrupadas\cite{breiman2017classification}.

          Considerando os dados no nó $m$ que pode ser representado pela letra $Q$. Para cada candidato é dividido $\theta = (j,t_{m})$ consistindo 
          a entrada como $j$ e o \textit{threshold} $t_{m}$, dividindo os dados entre $Q_{left}(\theta)$ e $Q_{right}(\theta)$ e esses subconjuntos pode ser vistos na
          Equação \ref{eq:DT}


          \begin{equation}\label{eq:DT}
            \begin{aligned}
              & Q_{left}(\theta) = (x,y)|x_{j} \leq t_{m} \\
              & Q_{right}(\theta) = Q \textbackslash Q_{left}(\theta)
          \end{aligned}
          \end{equation}


          A impureza no nó apresentado é calculada utilizando uma função de impureza denotada de $K()$, que pode ser escolhida dependendo se o 
          problema for de classificação ou regressão. Na Equação \ref{eq:DT2} é calculada essa impureza:

          \begin{equation}\label{eq:DT2}
            \begin{aligned}
              G(Q,\theta) = \frac{n_{left}}{N_{m}}K(Q_{left}(\theta)) + \frac{n_{right}}{N_{m}}K(Q_{right}(\theta))
          \end{aligned}
          \end{equation}


          Por fim, na Equação \ref{eq:DT3} são selecionados os parâmetros para diminuir a impureza,

          \begin{equation}\label{eq:DT3}
            \begin{aligned}
              \theta = \arg\min_{\theta}G(Q,\theta)
          \end{aligned} 
          \end{equation}

          É possível gerar outros nós a partir dos subconjuntos de dados gerados para direita e para esquerda, 
          até que a profundidade máxima permitida seja atingida, $N_{m} < \min_{samples}$ ou $N_{m} = 1$.

        \paragraph{Regressão Logística}

          Apesar do nome, é um modelo de classificação linear e não de regressão como sugere o seu nome. Neste modelo, os dados
          são descritos, através de um único teste e são modeladas a partir de uma função logística \cite{nasrabadi2007pattern}.

          Seja a probabilidade P(X), na Equação \ref{eq:LogReg1}


          \begin{equation}\label{eq:LogReg1}
            \begin{aligned}
              & P(x) = \frac{1}{1+e^{-(B_{0}+B_{1}x)}} = \frac{e^{B_{0} + B_{1}}x}{1+e^{B_{0}+B_{1}}x }\\ \\
              & \frac{p(x)}{1-p(x)} = e^{B_{0}+B_{1}x}\\ \\
          \end{aligned} 
          \end{equation}
         
          Onde, $\Beta_{i}$ são calculados através da função de máxima verossimilhança. 

    \subsubsection{Aprendizado não-supervisionado}

          Nesse tipo de aprendizado não existe a necessidade de um vetor contendo os rótulos de cada medida, ou seja, não é 
          necessário avisar a máquina o tipo de cada dado. A essa não inserção de rótulos faz com que o algoritmo aprenda de acordo 
          com as características dos dados de entrada. Esse tipo de paradigma não tem certos benefícios que existem  no aprendizado supervisionado, 
          para o seu funcionamento, o modelo propõe hipóteses a partir do que foi inserido na entrada \cite{bonaccorso2017machine}. Esse tipo de algoritmo 
          é muito interessante em base de dados que ainda não foram exploradas e que necessita-se visualizar os dados agrupados.

          A Figura \ref{unsupervised} é ilustrado o exemplo que foi exposto ao falar sobre aprendizado supervisionado, mas agora levando em consideração o 
          não-supervisionado. E é possível ver que o modelo foi capaz de agrupar as características semelhantes que os cachorros têm e agrupá-los em um
          único cluster e também foi formado um outro cluster contendo apenas o gato, pois não tem características semelhantes aos cães.

          \figuraBib{unsupervised}{Treinamento de um modelo de aprendizado de máquina não-supervisionado}{}{unsupervised}{width=0.6\textwidth}%

          \paragraph{K-means}
          O algoritmo mais utilizado no aprendizado não-supervisionado é o \textit{k-means}, pois a sua implementação é muito simples, pois basta comprovar
          o processo de minimizacão da distância quadrática total de cada ponto de um grupo, em relação ao centróide de referência \cite{macqueen1967some}. Na 
          Equação \ref{kmeans} é possível visualizar como é realizada a seleção de clusters através do número de amostras $X$, descrito pela média $\mu_{j}$ de 
          cada amostra nos clusters. 

          \begin{equation}\label{kmeans}
            \begin{aligned}
              \sum_{i=0}^{n}\min_{\mu_j \in C}(||x_i - \mu_j||^2)
          \end{aligned} 
          \end{equation}



    \subsubsection{Aprendizado por reforço}

          O último paradigma é baseado na forma em que o modelo reage de acordo com o que o ambiente passa para ele, é possível aprender
          continuamente. Esse aprendizado é realizado através das ações que são executadas, caso seja a decisão correta o modelo ganha uma recompensa, caso
          contrário pode ser imposta algum tipo de penalidade \cite{zurada}. Esse paradigma de aprendizado é necessário ajustar os parâmetros de forma
          contínua com o intuito de maximar ou minimizar um determinado índice, e sempre há um "treinador" nessa fase verificando se os estímulos proporcionados
          estão tendo a saída correta, pois caso esses dados não estejam balanceados, eles podem alterar a performance do algoritmo \cite{mazurowski2008training}.


          Na aprendizagem por reforço, o modelo só aprende com uma série de estímulos, que podem ser recompensas ou punições no decorrer da aprendizagem, 
          um exemplo do mundo real seria, avaliar um motorista com nota baixa em um aplicativo de locomoção urbana, nesse caso ele irá saber que existiu
          alguma condunta ruim. Um caso em que recebe um estímulo positivo pode ser dar um biscoito a um cachorro após fazer de forma correta o que foi pedido.

          Na Figura \ref{reforco} é possível ver um fluxograma de treinamento de um modelo utilizando o reforço.

          \figuraBib{reforco}{Fluxograma de treinamento de um modelo de aprendizado de máquina por reforço}{}{reforco}{width=0.6\textwidth}%

  \subsection{Medidas de avaliação}
          Ao utilizar um modelo \acrshort{AM} é necessário que existam parâmetros para demonstrar se o modelo está correto ou não. Pois existem 
          possibilidade do modelo ter um \textit{overfitting} que é quando o modelo se ajusta perfeitamente ao conjunto de dados de teste, mas não é capaz
          de prever novos resultados, já no \textit{underfitting} é quando não há dados suficientes ou com baixa qualidade e não é possível treinar o modelo 
          da maneira correta \cite{van2010process}.
          
          Na validação cruzada, os dados são dividos de forma aleatória em $n$ grupos de tamanho aproximadamente iguais. E este processo 
          é realizado por várias vezes, e em cada rodada do teste é considerado um novo valor. O seu desempenho é calculado a partir da média
          dos desempenhos calculados em cada uma das divisões, o número que $n$ pode assumir é variável, depende da situação, os valores
          mais utilizados são $5$ e $10$\cite{kohavi1995study}


          A matriz de confusão é outra forma de verificar a qualidade do modelo, pois ela fornece métricas importantes 
          para medir o desempenho. Essa é baseada em classes relacionadas à assertividade da previsão das classes. Os positivos verdadeiros (VP) e verdadeiros negativos (VN)
          que são os valores corretos que o sistema previu de forma correta, e também existe outras duas medidas que são os falsos negativos (FN) e falsos positivos (FP) que são 
          as entradas que foram classificadas de forma errada e tiveram sua saída trocada \cite{martin}. Na Figura \ref{confusion_matrix} é possível visualizar
          as classes de uma matriz de confusão.


          \figuraBib{confusion_matrix}{Matriz de confusão \cite{martin}}{}{confusion_matrix}{width=0.6\textwidth}%


          Com os resultados extraídos da matriz de confusão é possível calcular outras métricas para verificar o desempenho de cada algoritmo, 
          na Equação \ref{acuracia} é calculada a acurácia do modelo, na Equação \ref{precision} é calculada a precisão do modelo, na Equação \ref{recall} é
          calculada a frequência que o modelo encontra os exemplos de uma determinada classe e por fim na Equação \ref{f1score}, temos o F1-score que é uma métrica
          que indica a qualidade geral do modelo.

          \begin{equation}\label{acuracia}
            \begin{aligned}
              \textup{Acurácia} = \frac{VP+VN}{VP+VN+FP+FN}
          \end{aligned} 
          \end{equation}

          \begin{equation}\label{precision}
            \begin{aligned}
              \textup{Precisão} = \frac{VP}{VP+VN+FP+FN}
          \end{aligned} 
          \end{equation}

          \begin{equation}\label{recall}
            \begin{aligned}
              \textup{Recall} = \frac{VP}{VP+FN}
          \end{aligned} 
          \end{equation}


          \begin{equation}\label{f1score}
            \begin{aligned}
              \textup{F$1$-Score} = \frac{2*\textup{Precisão}*\textup{Recall}}{\textup{Precisão}+\textup{Recall}}
          \end{aligned} 
          \end{equation}

\section{Mineração de Texto}

          A \acrshort{TM} representa uma parcela da mineração de dados, também pode ser descrita como a forma 
        de descobrir padrões em textos, que vão desde identificar palavras até mesmo sumarizar o texto ou realizar análise de sentimentos
        de determinada informação. A sua análise tenta recolher o maior número de informações possíveis para encontrar padrões úteis e também 
        pode ser utilizada juntamente com algoritmos de \acrshort{AM}. Os textos geralmente são mais difíceis de obter análises, pois em grande
        parte não são apresentados de forma estruturada, portanto para que o resultado seja satisfatório é necessário uma etapa de pré-processamento
        dos dados, para que de alguma forma seja possível estruturar o texto.

            O corpus é o conjunto de frases que foram rotulados com a sua saída que será utilizada em um modelo de \acrshort{AM}, ou seja ele é 
        a base de dados que contém todos os textos disponíveis de forma rotulada.

  \subsection{Processamento de Linguagem Natural}

          O \acrshort{PLN} é uma área de pesquisa que preocupa-se em explorar como um computador pode entender um texto escrito por humanos, com esse tipo 
          de ferramenta é possível realizar análises morfológicas, sintáticas ou léxicas no texto e também extrair informações de um determinado
          documento \cite{liddy2001natural}. 

          \subsubsection{Análise de Sentimentos}

          A análise de sentimentos é um campo que envolve outras ciências além da computação, como a psicologia e linguística, pois é necessário 
          a construção de dicionários para facilitar a classificação de textos, quando por exemplo a intenção é apenas identificar a polaridade
          de uma palavras, ela poderá ter três saídas conhecidas: Positivo, Negativo e Neutro \cite{wegrzyn2012tweets}. 
          
          Existem diversas formas de fazer o uso da análise de sentimentos, a realização dessas tarefas no idioma em português ainda é muito díficil devido 
          não ter dicionários grandes para realizar o tratamento com um grande volume de dados, também é possível verificar a polaridade de cada frase através
          da bibliotca \cite{textblob}, mas para isso é necessário traduzir toda a frase para o idioma inglês através da \acrshort{API} aberta da \textit{Google},
          o que pode implicar um viés durante o processo de análise de sentimentos.
          
          É possível utilizar um modelo de \acrshort{AM} para realizar a análise de sentimentos, onde faz o uso do modelo previamente treinado para realizar a predição
          da polaridade de novas frases que não existiam durante a fase de treinamento \cite{golbeck2010twitter}.
          
          
        \subsubsection{Pre-Processamento}

            O pré-processamento dos dados é a parte mais importante da análise, pois é a etapa onde irá ser tratada as informações e com isso 
          facilitará o processo de aprendizado da máquina. Onde será indentificado padrões para facilitar a limpeza dos dados, onde pode ser definido
          que todo o texto deverá estar escrito em letra minúscula, remoção de pontuação e acentos, nessa fase também é possível retirar os textos que não 
          estão no padrão desejado, por exemplo apresentam \textit{emotions} ao invés de texto, remoção de números, ou seja, o interesse dessa etapa é 
          filtrar o texto de forma que a máquina obtenha apenas informações que irão ser utilizadas para treinamento \cite{ferreirametodo}.

        \subsubsection{Tokenização}

              Acontece na etapa de pré-processamento e a sua utilização tem como finalidade extrair pequenas informações de um texto livre. Recebe esse nome, 
            pois cada divisão é nomeada de \textit{token} e que na maioria das vezes é uma palavra que está inserida no texto, mas em outros casos podem 
            ser separados em letras ou duplas ou trincas de palavras\cite{mcnamee2004character}. Nesse trabalho foi utilizado o \textit{token} em sua forma clássica utilizando 
            a linguagem de programação \textit{Python} e a biblioteca \textit{NLTK} \cite{bird2004nltk}.

              Por exemplo, caso tenhamos a frase \textit{"Obina é melhor do que Neymar!"}, teremos sete tokens, conforme exemplo abaixo.

                    $[Obina]$  $[é]$  $[melhor]$  $[do]$  $[que]$  $[Neymar]$  $[!]$
        \subsubsection{Stemming}

            Essa técnica serve para evitar que palavras que tenham o mesmo radical sejam classificadas de maneira distintas, ou seja, essa técnica tem
            por objetivo reduzir até a menor parte com significado da palavra que no caso é o radical, portanto no processo de \textit{steeming}, palavras
            como caminhar, caminhada, caminharam, caminharão resultem na mesma palavra final: caminh. A necessário atentar-se na escolha do algoritmo correto 
            para realizar \textit{steeming}, pois essa etapa está diretamente ligada com o idioma em que o texto está escrito.
        \subsubsection{Stop Words}

          São os termos não relevantes para a análise do texto, são palavras que na maioria das vezes são conectivos de frases ou preposições, que podem
          ser consideradas como palavras vazias, não existe uma lista universal de \textit{Stop Words}, mas por padrão é utilizada a lista presente na biblioteca
          NLTK \cite{bird2004nltk}. Na Tabela \ref{tb:stopwords} encontra-se algumas das \textit{Stop Words} utilizadas nesse trabalho.

          \begin{table}[h]\label{tb:stopwords}
            \centering
            \begin{tabular}{cccccccc}
            \cline{1-8}
            de & os & tua & tem & estão & da & lhes & essas \\
            e & é & foi & nossas & muito & o & se & tuas \\
            tu & por & as & sua & aquele & entre & não & ele \\
            delas & minhas & às & nos & pela & havia & me & como \\
            ser & aqueles & nossa & vocês & eu & ter & tenho & suas \\
            está & isso & pelos & estes & tinha & depois & foram & este \\
            para & só & quem & deles & isto & um & eles & do \\
            vos & mais & mesmo & num & dele & será & minha & a \\
            no & teus & à & você & em & meus & esses & pelas \\
            com & ao & dela & há & que & na & nosso & te \\
            aos & dos & ou & aquela & era & uma & das & esta \\
            teu & nem & já & até & seja & esse & mas & quando \\
            aquelas & nossos & têm & também & seus & lhe & meu & seu \\
            ela & elas & estas & nós & sem & essa & fosse & qual \\
            & & pelo & nas & numa & aquilo & & \\
            \cline{1-8}
            \end{tabular}
            \caption{\textit{Stop Words} utilizadas nesse trabalho}
            \label{exemplos-stop-words}
            \end{table}
            
  

  \subsection{Bag of Words}
  \subsection{Term Frequency}
  \subsection{Term Frequency - Inverse Document Frequency (TF-IDF)}




