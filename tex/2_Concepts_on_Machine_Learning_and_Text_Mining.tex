Neste capítulo é feita uma introdução sobre o \acrshort{AM}, tema principal dessa monografia. E está dividido da seguinte
maneira na seção 2.1 são apresentados os conceitos básicos para o entendimento inicial do tema. Na seção 2.2 são apresentados
os conceitos de mineração de texto. Na seção 2.3 é apresentado o conceito de análise de sentimentos e a sua aplicação.


\section{Machine Learning}
  \subsection{Basic Concepts}

    Com o alto volume de informações geradas no dia a dia, a utilização de algoritmos que sejam capazes de identificar padrões tornam-se cada
    vez mais necessários, pois em diversas empresas e órgãos do governo não são capazes de analisar todas as informações existentes ou entregar
    de maneira célere \cite{nasrabadi2007pattern}. 
    
    Portanto para realizar esse tipo de atividade é necessário entender o problema existente e também ter um volume de dados razoável para realizar o treinamento do modelo, e com a técnica de \acrshort{AM} é possível automatizar a construção
    de sistemas inteligentes que podem ser ajustados de acordo com a necessidade de cada tarefa \cite{bonaccorso2017machine}.

    Em outras palavras o \acrshort{AM} é um conjunto de regras, que possibilitam uma máquina a tomar decisões baseadas em experiências passadas ao invés
    de um software que teve que ser definido anteriormente como tratar cada tipo de regra, também existe a possibilidade desses modelos serem desenvolvidos
    e melhorarem quando expostos a novos dados \cite{zurada1995review}.


    O conceito de \acrshort{AM} pode ser sintetizado como a capacidade de um programa de computador aprender com a experiencia (E) relacionada a alguma 
    classe de tarefas (T), baseada em uma medida de desempenho (P). Dessa forma, o desempenho em tarefas (T), quando medido por (P), melhora com a 
    experiencia em (E) \cite{mitchell}

    Essas técnicas tem sido utilizadas amplamente para resolução de diversos problemas, atualmente o assunto está em alta e existem diversas oportunidade
    para colocar em prática a matemática que foi desenvolvida para a criação desses algoritmos. Gigantes da indústria tem investido severamente para
    o desenvolvimento de novas tecnologias, como os veículos autônomos, robôs advogados, veículos não tripulados e na identificação de doenças.

    Existem distintos paradigmas para ensinar uma máquina, esse trabalho irá focar apenas nos tipos de \acrshort{AM} que estão sendo utilizados no desenvolvimento do framework,
    portanto não será abordados temas como aprendizado estatístico ou redes bayesianas.

  \subsection{Paradigmas de Aprendizado de Máquina}

    Os principais tipos de \acrshort{AM} utilizados atualmente serão detalhados nas 3 subseções a seguir:

    \subsubsection{Aprendizado Supervisionado}

      O aprendizado supervisionado é uma das formas em ensinar uma tarefa para a máquina, onde existe uma entrada e uma saída desejada que já foi 
      anteriormente rotulada, esse processo pode ser feito de forma manual ou utilizar de dicionários, quando a informação de entrada é um texto.
      Tendo os dados detalhados a máquina é capaz de classificar novas entradas a partir de experiências antigas \cite{mitchell}. Na Figura \ref{supervisionado} é possível visualizar 
      a forma que é realizada etapa de treinamento do modelo, onde é fornecida uma informação, juntamente com o que ela significa, pois esse tipo de aprendizado 
      é necessário rotular todas as informações que serão utilizadas para que a máquina consiga distinguir o que é gato e o que é cachorro.
      

      \figuraBib{supervisionado}{Treinamento de um modelo de aprenzidado de máquina supervisionado}{}{supervisionado}{width=.6\textwidth}%

      Na Figura \ref{supervisionado_predicao} é possível visualizar o modelo preditivo em funcionamento, onde o usuário fornece uma informação de entrada
      e o sistema o responde com a classe que foi identificada através da entrada.

      \figuraBib{supervisionado_predicao}{Utilização do modelo para realizar predição a partir de classes previamente treinadas}{}{supervisionado_predicao}{width=.6\textwidth}%


      Algum dos algoritmos mais utilizados para esse paradigma serão detalhados durante essa subseção.

      \paragraph{SVM}

        A máquina de vetor de suportes, do inglês support vector machine (\acrshort{SVM}), faz parte do aprendizado supervisionado, com ele é possível
        classificar grupos de dados separando as suas margens. Essas margens são delineadas pela fração dos dados de treinamento, são chamadas de vetores de suporte \cite{chang2011libsvm}.

        As vantagens de utilizar \acrshort{SVM} são \cite{pedregosa2011scikit}:

        \begin{itemize}

          \item Efetivos em espaços multidimensionais
          \item Continua eficaz em casos onde o número de dimensões é maior que o número de amostras.
          \item Utiliza os vetores de suporte, que otimiza o uso de memória do computador.
          \item É possível utilizar diversos kernels para a função de decisão.

        \end{itemize}

        Como desvantagens, temos \cite{pedregosa2011scikit}:

        The disadvantages of support vector machines include:

        \begin{itemize}

          \item Se o número de entradas for muito maior que o número de amostras, pois existe a possibilidade de overfitting.

        \end{itemize}
        
        O \acrshort{SVM} é construido em um hiper-plano ou em vários hiper-planos em um espaço de infinitas dimensões, que podem ser
        usadas classificação ou regressão. Na Figura \ref{SVM} é detalhado um hiper-plano que mostra uma boa separação das variáveis
        que possui a maior distância até os pontos dos dados de treinamento mais próximos de qualquer classe, pois é conhecido no \acrshort{SVM}
        que quanto maior for a margem, menor será o erro do classificador\cite{vieira2017plantrna_sniffer}. 
        
        \figuraBib{SVM}{Exemplo de vetores de suporte com dimensão 2}{}{SVM}{width=.6\textwidth}%

        Nesse trabalho foi utilizado esse algoritmo para classificar dados multi-classes, pois foi utilizadas três classes distintas, para
        desenvolvimento do modelo: Positivo, Negativo e Neutro. 
        
        Tendo como os dados de treinamento $x_{i} \in \mathbb{R}^{p}$, onde i$=$1,\cdots, n. Onde $ y \in \left \{ 1,-1 \right \}^{n}$, na Equação \ref{eq:SVM} é
        possível visualizar a solução matemática para esse algoritmo.

        \begin{equation}\label{eq:SVM}
          \begin{aligned}
            & \min_{\omega,\beta, \zeta } \frac{1}{2}\omega^{T}\omega + C \sum_{i=1}^{n}\zeta_{i}\\
            & \textup{sujeito a } y_{i}\left ( \omega^{T}\phi \left ( x_{i} \right ) +b \right )\geq 1 - \zeta_{i},\\
            & \zeta_{i}\geq 0, i = 1,\cdots, n
        \end{aligned}
        \end{equation}

        A partir do espaço de Hilbert, temos que o dual de um espaço de Hilbert é um espaço de Hilbert \cite{lorena2007introduccao}, 
        
        na Equação \ref{eq:svmdual}, temos:

        \begin{equation}\label{eq:svmdual}
          \begin{aligned}
            & \min_{\alpha} \frac{1}{2}\alpha^{T}Q\alpha - e^{T}\alpha\\ \\
            & \textup{sujeito } a y^{T}\alpha = 0 \\ \\
            & 0\leq \alpha_{i}\leq C, i= 1\cdots, n 
        \end{aligned}
        \end{equation}

        onde $e$ é o vetor com todos os valores, $C>0$ é o limite superior, $Q$ é uma matriz semidefinida positiva de tamanho n por n,
        $Q_{ij}\equiv y_{i}y_{j}K(x_{i},x_{j})$, onde $K(x_{i},x_{j}) = \phi(x_{i})^{T}\phi(x_{j})$ é a função kernel do \acrshort{SVM}. Onde os 
        vetores de treinamento são definidos em um espaço dimensional maior pela função $\phi$.
         E por fim na Equação \ref{eq:predSVM}, temos a função de decisão:

         \begin{equation}\label{eq:predSVM}
          \begin{aligned}
            sgn(\sum_{i=1}^{n}y_{i}\alpha_{i}K(x_{i},x)+\rho )
        \end{aligned}
        \end{equation}
        
        Onde o termo $y_{i}\alpha_{i}$ representa o coeficiente dual, $K(x_{i},x)$ são os vetores de suporte e $\rho$ é um termo independente.



        \paragraph{Naive Bayes}

        O Naive Bayes é um outro algoritmo \acrshort{AM} supervisionado, a sua formulação matemática é sustentada pelo teorema estatístico de
        Bayes com um pouco de ingenuidade, pois assume-se que a suposição de independência condicional entre cada par de recursos, dado o valor da
        variável de classe \cite{McCallum98acomparison}. O teorema de Bayes clássico segue a seguinte relação, onde um termo independente $y$ e o vetor $x_{1}$ até $x_{n}$, na Equação
        \ref{eq:Bayes}

        \begin{equation}\label{eq:Bayes}
          \begin{aligned}
            P\left ( y\mid x_{1},\cdots, x_{n} \right ) = \frac{P(y)P(x_{1},\cdots,x_{n}\mid y)}{P(x_{1},\cdots, x_{n})}
        \end{aligned}
        \end{equation}
        

        Na Equação \ref{eq:Bayes1} é desconsiderado completamente a correlação entre as variáveis, 


        \begin{equation}\label{eq:Bayes1}
          \begin{aligned}
            P\left ( x_{i}\mid y,x_{1}, \cdots,x_{i-1},x_{i+1},\cdots, x_{n} \right ) = P(x_{i}|y),
        \end{aligned}
        \end{equation}


        Na Equação \ref{eq:BayesSimples}, para todo $i$, é possível simplificar através da relação:


        \begin{equation}\label{eq:BayesSimples}
          \begin{aligned}
            P(y|x_{1},\cdots, x_{n}) = \frac{P(y)\prod_{i=1}^{n}P(x_{i}|y)}{P(x_{1},\cdots, x_{n})},
        \end{aligned}
        \end{equation}


        Sabendo que $P(x_{1},\cdots, x_{n})$ é constante dada a entrada, podemos usar a seguinte regra de classificação, na Equação \ref{eq:NBFINAL}:
        
        \begin{equation}\label{eq:NBFINAL}
          \begin{aligned}
            \widehat{y} = \arg\max_{y} P(y)\prod_{i=1}^{n}P(x_{i}|y),
        \end{aligned}
        \end{equation}


        O classificador Naive Bayes pode ser extremamente rápido em comparação a outros algoritmos mais sofisticados, pois existe o desacoplamento
        das distribuições de características condicionais de classe que significa que cada uma pode ser estimada independentemente com uma distribuição unidimensional\cite{zhang2004optimality}.


        \paragraph{Árvores de Decisão}

          A árvore de decisão funciona quando as classes presentes no conjunto de dados de treinamento se dividem, ou seja, esse algoritmo seleciona
          um problema mais complexo e divide em subproblemas mais simples e de forma recursiva a mesma estratégia é a aplicada a cada subproblema \cite{coelhoanalise}.


          Na Figura \ref{decisionTree}, é um exemplo de como uma árvore de decisão poderia realizar uma análise de crédito de forma rápida. 


          \figuraBib{decision_tree}{Árvore de decisão para análise de crédito de um cliente que deseja empréstimo alto no banco, levando em consideração 
          a educação e faixa salarial}{}{decisionTree}{width=\textwidth}%


          Tendo como entrada os vetores de treinamento $x_{i} \in \mathbb{R}^{n}$, $i=1,\cdots, n$, e o vetor com os rótulos de saída $y \in \mathbb{R}^{n}$,
          a árvore de decisão divide recursivamente o espaço de forma que as amostras com os mesmos rótulos sejam agrupadas\cite{breiman2017classification}.

          Considerando os dados no nó $m$ que pode ser representado pela letra $Q$. Para cada candidato é dividido $\theta = (j,t_{m})$ consistindo 
          a entrada como $j$ e o \textit{threshold} $t_{m}$, dividindo os dados entre $Q_{left}(\theta)$ e $Q_{right}(\theta)$ e esses subconjuntos pode ser vistos na
          Equação \ref{eq:DT}


          \begin{equation}\label{eq:DT}
            \begin{aligned}
              & Q_{left}(\theta) = (x,y)|x_{j} \leq t_{m} \\
              & Q_{right}(\theta) = \frac{Q}{Q_{left}(\theta)}
          \end{aligned}
          \end{equation}


          A impureza no nó apresentado é calculada utilizando uma função de impureza denotada de $K()$, que pode ser escolhida dependendo se o 
          problema for de classificação ou regressão. Na Equação \ref{eq:DT2} é calculada essa impureza:

          \begin{equation}\label{eq:DT2}
            \begin{aligned}
              G(Q,\theta) = \frac{n_{left}}{N_{m}}K(Q_{left}(\theta)) + \frac{n_{right}}{N_{m}}K(Q_{right}(\theta))
          \end{aligned}
          \end{equation}


          Por fim, na Equação \ref{eq:DT3} são selecionados os parâmetros para diminuir a impureza,

          \begin{equation}\label{eq:DT3}
            \begin{aligned}
              \theta = \arg\min_{\theta}G(Q,\theta)
          \end{aligned} 
          \end{equation}

          É possível gerar outros nós a partir dos subconjuntos de dados gerados para direita e para esquerda, 
          até que a profundidade máxima permitida seja atingida, $N_{m} < \min_{samples}$ ou $N_{m} = 1$.

        \paragraph{Regressão Logística}

          Apesar do nome, é um modelo de classificação linear e não de regressão como sugere o seu nome. Neste modelo, os dados
          são descritos, através de um único teste e são modeladas a partir de uma função logística \cite{nasrabadi2007pattern}.

          Seja a probabilidade P(X), na Equação \ref{eq:LogReg1}


          \begin{equation}\label{eq:LogReg1}
            \begin{aligned}
              & P(x) = \frac{1}{1+e^{-(B_{0}+B_{1}x)}} = \frac{e^{B_{0} + B_{1}}x}{1+e^{B_{0}+B_{1}}x }\\ \\
              & \frac{p(x)}{1-p(x)} = e^{B_{0}+B_{1}x}\\ \\
          \end{aligned} 
          \end{equation}
         
          Onde, $\Beta_{i}$ são calculados através da função de máxima verossimilhança. 

    \subsubsection{Aprendizado não-supervisionado}

          Nesse tipo de aprendizado não existe a necessidade de um vetor contendo os rótulos de cada medida, ou seja, não é 
          necessário avisar a máquina o tipo de cada dado. A essa não inserção de rótulos faz com que o algoritmo aprenda de acordo 
          com as características dos dados de entrada. Esse tipo de paradigma não tem certos benefícios que existem  no aprendizado supervisionado, 
          para o seu funcionamento, o modelo propõe hipóteses a partir do que foi inserido na entrada \cite{bonaccorso2017machine}. Esse tipo de algoritmo 
          é muito interessante em base de dados que ainda não foram exploradas e que necessita-se visualizar os dados agrupados.

          A Figura \ref{unsupervised} é ilustrado o exemplo que foi exposto ao falar sobre aprendizado supervisionado, mas agora levando em consideração o 
          não-supervisionado. E é possível ver que o modelo foi capaz de agrupar as características semelhantes que os cachorros têm e agrupá-los em um
          único cluster e também foi formado um outro cluster contendo apenas o gato, pois não tem características semelhantes aos cães.

          \figuraBib{unsupervised}{Treinamento de um modelo de aprenzidado de máquina não-supervisionado}{}{unsupervised}{width=0.6\textwidth}%
    \subsubsection{Reinforcement Learning}


  \subsection{Performance measures}


\section{Text Mining}
  \subsection{Natural Language Processing}
  \subsection{Pre-Processing}
  \subsection{Tokenization}
  \subsection{Stemming}
  \subsection{Stop Words}
  \subsection{Bag of Words}
  \subsection{Term Frequency}
  \subsection{Term Frequency - Inverse Document Frequency (TF-IDF)}

\section{Sentiment Analysis}

